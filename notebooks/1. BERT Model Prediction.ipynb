{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "import torch\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time \n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "\n",
    "### configure the BERT network \n",
    "\"\"\"\n",
    "Two models available, one is uncased, the other one is cased, \n",
    "Change according to the importance of CASE in the sentence\n",
    "\n",
    "\"\"\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()\n",
    "\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# bert_model = BertForMaskedLM.from_pretrained('bert-base-cased').eval()\n",
    "\n",
    "\"\"\"\n",
    "Input network using Gelphic\n",
    "\"\"\"\n",
    "# network = pd.read_csv(\"MLDA Synonym.csv\")\n",
    "Gelphi_output_file_path = \"../source_data/Gelphi_output_1.csv\" \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "corpus input path to read the original text\n",
    "\"\"\"\n",
    "corpus_input_csv_path = '../source_data/corpus_input.csv'\n",
    "\n",
    "\"\"\"\n",
    "output_name file name \n",
    "\"\"\"\n",
    "bert_with_network_score_file_name=\"../output/bert_final_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing to fit the BERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the predicted words from  BERT model\n",
    "def decode(tokenizer, pred_idx, top_clean=5):\n",
    "    ignore_tokens = string.punctuation + '[PAD]'\n",
    "    tokens = []\n",
    "    for w in pred_idx:\n",
    "        token = ''.join(tokenizer.decode(w).split())\n",
    "        if token not in ignore_tokens:\n",
    "            tokens.append(token.replace('##', ''))\n",
    "    return tokens[:top_clean]  ## each line one prediction\n",
    "\n",
    "### Encode the words by BERT tokenizers\n",
    "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
    "    text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
    "    # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "    if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "        text_sentence += ' .'\n",
    "    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "    return input_ids, mask_idx\n",
    "\n",
    "###\n",
    "def get_predictions(text_sentence,top_clean=5):\n",
    "    input_ids,mask_idx = encode(bert_tokenizer,text_sentence)\n",
    "    with torch.no_grad():\n",
    "        predict= bert_model(input_ids)[0]\n",
    "    bert = decode(bert_tokenizer,predict[0,mask_idx,:].topk(top_clean).indices.tolist(),top_clean=top_clean)\n",
    "    return bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text pre-processing functions\n",
    "To split those cut words with come with punctuation e.g. \"like.\", \". enable\"\n",
    "\"\"\"\n",
    "\n",
    "def punctuation_corr(input_sent):\n",
    "    ## correct punctuation position\n",
    "    input_split = input_sent.split()\n",
    "    for i in range(len(input_split)):\n",
    "        if not input_split[i]: ## for \\t\\n char \n",
    "            continue\n",
    "        ## word starts with a punctuation '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' but not @ => @abc , may be a tweet mention\n",
    "        if input_split[i][0] in string.punctuation and input_split[i][0]!='@':\n",
    "            input_split[i]= input_split[i][0] + \" \" + input_split[i][1:]\n",
    "        \n",
    "        if input_split[i][-1] in string.punctuation:\n",
    "            input_split[i]= input_split[i][:-1] + \" \" + input_split[i][-1]\n",
    "            if input_split[i][-3] in string.punctuation: ## for .\" \n",
    "                orig_punc = input_split[i][-3]\n",
    "                input_split[i]= input_split[i][:-3] +\" \"+input_split[i][-3:]\n",
    "                \n",
    "        ### account for all CAP words, convert to lower case\n",
    "        elif input_split[i].upper() == input_split[i] and len(input_split[i])> 1 :  \n",
    "            input_split[i] = input_split[i].lower()\n",
    "        \n",
    "        ## for punct in between the words without space e.g.\"buy,i\"\n",
    "\n",
    "        try:\n",
    "            punc_pos = len(re.search('\\w+',input_split[i])[0])\n",
    "            if punc_pos<len(input_split[i])-1:\n",
    "                if input_split[i][punc_pos] in string.punctuation and input_split[i][punc_pos] !='-' and input_split[i][punc_pos] !=\"'\" :\n",
    "                    input_split[i] = input_split[i][:punc_pos] + \" \" +input_split[i][punc_pos]+\" \"+input_split[i][punc_pos+1:]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    input_sent = ' '.join(input_split)\n",
    "    return input_sent\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Based on the cleaned text, mask out interested words for BERT prediction and get the BERT prediction\n",
    "Two ways to split the sentence, by Spacy or NLTk (Spacy is more advanced and time-consuming)\n",
    "\n",
    "input_sent (string): the corpus for prediction\n",
    "top_k (int): number of prediction get from BERT model for each masked word \n",
    "useSpacy (boolean): whether the use of Spacy to split words and pos tagging\n",
    "\"\"\"\n",
    "\n",
    "def find_masked_words(input_sent,top_k=5, useSpacy=True):\n",
    "    keyword = defaultdict(dict)\n",
    "    if not useSpacy:\n",
    "        input_sent = punctuation_corr(input_sent)\n",
    "        input_split = input_sent.split()\n",
    "            ## second filter with pos tagging for nltk\n",
    "        ### POS TAGGGING \n",
    "        not_consider_pos = ['PRON','DET','ADP','CONJ','NUM','PRT','.',':','CC','CD','EX','DT','PDT',\n",
    "                        'IN','LS','MD','NNP','NNPS','PRP','POS','PRP$','TO','UH','WDT','WP$','WP','WRB']\n",
    "            #  refer to   https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/ \n",
    "        be_do_verb = ['is','are','was','were','did','does','do','not','had','have','has','ever']\n",
    "        conjunction_words = ['therefore','thus']\n",
    "        pos_res = nltk.pos_tag(input_sent.split())\n",
    "        for item_num in range(len(pos_res)):\n",
    "            if pos_res[item_num][1] not in not_consider_pos:\n",
    "                ## another level of filtering of words before masking \n",
    "                if '@' in pos_res[item_num][0]:\n",
    "                    continue\n",
    "                if pos_res[item_num][0].lower() in be_do_verb:\n",
    "                    continue\n",
    "                if pos_res[item_num][0].lower() in conjunction_words:\n",
    "                    continue\n",
    "                if pos_res[item_num][0][-2:] in [\"'s\",\"'t\",\"'r\",\"'d\",\"'l\",\"'v\",\"'m\"]:\n",
    "                    continue\n",
    "                if 'http' in pos_res[item_num][0]:\n",
    "                    continue\n",
    "                if len(pos_res[item_num][0])<3:\n",
    "                    continue\n",
    "                if pos_res[item_num][0] in stop_words:\n",
    "                    continue\n",
    "                if pos_res[item_num][0][-1] in string.punctuation:\n",
    "                    pos_res[item_num] = pos_res[item_num][:-1]\n",
    "                orig = input_split[item_num]\n",
    "                input_split[item_num]='<mask>'\n",
    "                input_text_for_pred = ' '.join(input_split)\n",
    "                input_split[item_num]=orig\n",
    "                keyword[pos_res[item_num][0]+ \"_\"+str(item_num)]['prediction']=get_predictions(input_text_for_pred, top_clean=top_k)\n",
    "    else:\n",
    "        doc = nlp(input_sent)\n",
    "        input_split = doc.text.split()\n",
    "        ### reg_exp to detect punctuation and number in the word splits\n",
    "        reg_exp= \"[\"+string.punctuation+\"0-9]\"\n",
    "        for i in range(len(input_split)):\n",
    "            \n",
    "            if len(doc[i].text)<3: ## skip words with length < 3\n",
    "                continue\n",
    "            if re.search(reg_exp,doc[i].text): ## skip punctuation and number\n",
    "                continue\n",
    "            ### remove words that are definitely not emo-denoting for easier computation\n",
    "            if not doc[i].is_stop and doc[i].pos_ not in ['SPACE','PUNCT','ADX','CONJ','CCONJ',\n",
    "                                                        'DET','INTJ','NUM','PRON','PROPN','SCONJ','SYM']:\n",
    "                orig = input_split[i]\n",
    "                input_split[i]= \"<mask>\"\n",
    "                input_text_for_pred = ' '.join(input_split) ### join the split words together with <mask> for BERT prediction\n",
    "                input_split[i]= orig\n",
    "                keyword[doc[i].text+ \"_\"+str(i)]['prediction']=get_predictions(input_text_for_pred, top_clean=top_k)\n",
    "            \n",
    "        \n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = pd.read_csv(Gelphi_output_file_path)\n",
    "network.set_index(network['Label'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Auxilary functions to find out the prediction from BERT model, change top_k_choic\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "### match score pertaining to the masked words with the network metrics/score\n",
    "## match_col : \"Authority\" , \"modularity_class\",\"Weighted Degree\",\"betweenesscentrality\"\n",
    "def self_score(match_col=\"Authority\",pred_out_pf=None):\n",
    "    return pred_out_pf['cleaned_index'].map(network[match_col].to_dict())\n",
    "\n",
    "### match score pertaining to the masked predictions with the network metrics/score\n",
    "## match_col : \"Authority\" , \"modularity_class\",\"Weighted Degree\",\"betweenesscentrality\"\n",
    "def pred_score(match_col=\"Authority\",pred_out=None):\n",
    "    pred_score_output = []\n",
    "    for item in pred_out['prediction']:\n",
    "        item = item.lower()\n",
    "        try:\n",
    "            pred_score_output.append(network[match_col].to_dict()[item])\n",
    "        except:\n",
    "            pred_score_output.append(-1)\n",
    "    return pred_score_output\n",
    "\n",
    "\n",
    "### aggregate function\n",
    "def key_word_predict_with_network_from_sent(input_sent=None,top_k=None, filter_NA_pred=True):\n",
    "    keyword_pred_from_bert_output = find_masked_words(input_sent, top_k=top_k)\n",
    "    res_out = pd.DataFrame(keyword_pred_from_bert_output).transpose()\n",
    "    res_out['cleaned_index']= [ item.split('_')[0].lower() for item in res_out.index]\n",
    "    res_out['Label'] = self_score(match_col=\"Label\",pred_out_pf=res_out)## check whether in the network\n",
    "    res_out['self_auth'] = self_score(match_col=\"Authority\",pred_out_pf=res_out)\n",
    "    res_out['self_class'] = self_score(match_col=\"modularity_class\",pred_out_pf=res_out)\n",
    "    res_out['self_deg'] = self_score(match_col=\"Weighted Degree\",pred_out_pf=res_out)\n",
    "    res_out['self_betcent'] = self_score(match_col=\"betweenesscentrality\",pred_out_pf=res_out)\n",
    "    res_out['pred_betcent'] = res_out.apply(lambda row: pred_score(match_col=\"betweenesscentrality\",pred_out=row),axis=1)\n",
    "    res_out['pred_auth'] = res_out.apply(lambda row: pred_score(match_col=\"Authority\",pred_out=row),axis=1)\n",
    "    res_out['pred_deg'] = res_out.apply(lambda row: pred_score(match_col=\"Weighted Degree\",pred_out=row),axis=1)\n",
    "    res_out['pred_class'] = res_out.apply(lambda row: pred_score(match_col=\"modularity_class\",pred_out=row),axis=1)\n",
    "    return res_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 824 done\n",
      "time taken 0:00:26.766007\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main function to generate final output \n",
    "\n",
    "\"\"\"\n",
    "corpus_input_csv_path = '../source_data/corpus_input.csv'\n",
    "start = datetime.datetime.now()\n",
    "df = pd.read_csv(corpus_input_csv_path) ### load the corpus text \n",
    "df=df.reset_index()\n",
    "res_new = pd.DataFrame()\n",
    "# for i in range(0,len(df)):\n",
    "for i in range(0,10):    \n",
    "    input_text = df['Text'].loc[i]\n",
    "    if i%100==0:\n",
    "        print(f'{i} / {len(df)} done')\n",
    "    res = key_word_predict_with_network_from_sent(input_text,top_k=5)\n",
    "    res['from_textid'] = df['index'].loc[i]\n",
    "    res_new = res_new.append(res)\n",
    "end =datetime.datetime.now()\n",
    "print(f'time taken {end-start}')\n",
    "res_new2=res_new.reset_index()\n",
    "res_new2.to_csv(bert_with_network_score_file_name+'.csv',index=False)\n",
    "print(f'{round(27/116,2)}s average time taken to predict each masked word' )\n",
    "print(f'BERT Final Prediction with score is saved in {bert_with_network_score_file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prediction</th>\n",
       "      <th>cleaned_index</th>\n",
       "      <th>Label</th>\n",
       "      <th>self_auth</th>\n",
       "      <th>self_class</th>\n",
       "      <th>self_deg</th>\n",
       "      <th>self_betcent</th>\n",
       "      <th>pred_betcent</th>\n",
       "      <th>pred_auth</th>\n",
       "      <th>pred_deg</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>from_textid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good_1</td>\n",
       "      <td>[the, new, little, old, more]</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "      <td>0.013005</td>\n",
       "      <td>18.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[-1, 2012.54884, 4296.3050299999995, 0.0, 4938...</td>\n",
       "      <td>[-1, 0.001431, 0.001866, 0.000349, 0.000708]</td>\n",
       "      <td>[-1, 59, 162, 66, 61]</td>\n",
       "      <td>[-1, 10, 6, 10, 9]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stooge_2</td>\n",
       "      <td>[comedy, movie, old, story, funny]</td>\n",
       "      <td>stooge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-1, 3307.571201, 0.0, 499.947545, 42958.099068]</td>\n",
       "      <td>[-1, 1e-05, 0.000349, 0.000131, 0.001755]</td>\n",
       "      <td>[-1, 6, 66, 68, 127]</td>\n",
       "      <td>[-1, 24, 10, 0, 10]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lovely_7</td>\n",
       "      <td>[good, evil, mean, young, bad]</td>\n",
       "      <td>lovely</td>\n",
       "      <td>lovely</td>\n",
       "      <td>0.101399</td>\n",
       "      <td>17.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.0, 776.6235889999999, 97091.279297, -1, 433...</td>\n",
       "      <td>[0.013005, 0.00123, 0.102042, -1, 0.004394]</td>\n",
       "      <td>[398, 130, 423, -1, 309]</td>\n",
       "      <td>[18, 11, 17, -1, 11]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evil_9</td>\n",
       "      <td>[happy, funny, young, beautiful, different]</td>\n",
       "      <td>evil</td>\n",
       "      <td>evil</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>11.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>776.623589</td>\n",
       "      <td>[64807.099323, 42958.099068, -1, 0.0, 10103.08...</td>\n",
       "      <td>[0.000502, 0.001755, -1, 0.101401, 7.8e-05]</td>\n",
       "      <td>[61, 127, -1, 216, 6]</td>\n",
       "      <td>[18, 10, -1, 17, 5]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time_13</td>\n",
       "      <td>[thing, way, person, as, woman]</td>\n",
       "      <td>time</td>\n",
       "      <td>time</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[37456.128994, 0.0, 71005.676086, 0.0, -1]</td>\n",
       "      <td>[0.000816, 0.016628, 0.000334, 0.000121, -1]</td>\n",
       "      <td>[172, 266, 61, 28, -1]</td>\n",
       "      <td>[3, 25, 3, 9, -1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>way_21</td>\n",
       "      <td>[dialogue, story, action, plot, movie]</td>\n",
       "      <td>way</td>\n",
       "      <td>way</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>25.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.0, 499.947545, 23449.4279, 0.0, 3307.571201]</td>\n",
       "      <td>[8e-06, 0.000131, 0.000116, 6.500000000000001e...</td>\n",
       "      <td>[7, 68, 39, 40, 6]</td>\n",
       "      <td>[0, 0, 6, 3, 24]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>movie_23</td>\n",
       "      <td>[so, just, even, exactly, being]</td>\n",
       "      <td>movie</td>\n",
       "      <td>movie</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3307.571201</td>\n",
       "      <td>[2087.67817, 0.0, 0.0, 62.2058, 0.0]</td>\n",
       "      <td>[0.015486000000000003, 0.003158, 0.00074699999...</td>\n",
       "      <td>[222, 168, 106, 83, 125]</td>\n",
       "      <td>[25, 18, 5, 5, 3]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>randomly_27</td>\n",
       "      <td>[also, just, definitely, even, all]</td>\n",
       "      <td>randomly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.000391, 0.003158, 0.000491, 0.0007469999999...</td>\n",
       "      <td>[33, 168, 43, 106, 90]</td>\n",
       "      <td>[9, 18, 21, 5, 5]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>silly_28</td>\n",
       "      <td>[had, sent, carried, left, got]</td>\n",
       "      <td>silly</td>\n",
       "      <td>silly</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[-1, -1, -1, -1, 149.284191]</td>\n",
       "      <td>[-1, -1, -1, -1, 0.001297]</td>\n",
       "      <td>[-1, -1, -1, -1, 20]</td>\n",
       "      <td>[-1, -1, -1, -1, 9]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>message_34</td>\n",
       "      <td>[me, you, people, us, everyone]</td>\n",
       "      <td>message</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-1, -1, 6685.643157, -1, -1]</td>\n",
       "      <td>[-1, -1, 0.000193, -1, -1]</td>\n",
       "      <td>[-1, -1, 67, -1, -1]</td>\n",
       "      <td>[-1, -1, 3, -1, -1]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index                                   prediction cleaned_index  \\\n",
       "0         good_1                [the, new, little, old, more]          good   \n",
       "1       Stooge_2           [comedy, movie, old, story, funny]        stooge   \n",
       "2       lovely_7               [good, evil, mean, young, bad]        lovely   \n",
       "3         evil_9  [happy, funny, young, beautiful, different]          evil   \n",
       "4        time_13              [thing, way, person, as, woman]          time   \n",
       "..           ...                                          ...           ...   \n",
       "111       way_21       [dialogue, story, action, plot, movie]           way   \n",
       "112     movie_23             [so, just, even, exactly, being]         movie   \n",
       "113  randomly_27          [also, just, definitely, even, all]      randomly   \n",
       "114     silly_28              [had, sent, carried, left, got]         silly   \n",
       "115   message_34              [me, you, people, us, everyone]       message   \n",
       "\n",
       "      Label  self_auth  self_class  self_deg  self_betcent  \\\n",
       "0      good   0.013005        18.0     398.0      0.000000   \n",
       "1       NaN        NaN         NaN       NaN           NaN   \n",
       "2    lovely   0.101399        17.0     226.0      0.000000   \n",
       "3      evil   0.001230        11.0     130.0    776.623589   \n",
       "4      time   0.000318         9.0      10.0      0.000000   \n",
       "..      ...        ...         ...       ...           ...   \n",
       "111     way   0.016628        25.0     266.0      0.000000   \n",
       "112   movie   0.000010        24.0       6.0   3307.571201   \n",
       "113     NaN        NaN         NaN       NaN           NaN   \n",
       "114   silly   0.001007        10.0      47.0      0.000000   \n",
       "115     NaN        NaN         NaN       NaN           NaN   \n",
       "\n",
       "                                          pred_betcent  \\\n",
       "0    [-1, 2012.54884, 4296.3050299999995, 0.0, 4938...   \n",
       "1     [-1, 3307.571201, 0.0, 499.947545, 42958.099068]   \n",
       "2    [0.0, 776.6235889999999, 97091.279297, -1, 433...   \n",
       "3    [64807.099323, 42958.099068, -1, 0.0, 10103.08...   \n",
       "4           [37456.128994, 0.0, 71005.676086, 0.0, -1]   \n",
       "..                                                 ...   \n",
       "111    [0.0, 499.947545, 23449.4279, 0.0, 3307.571201]   \n",
       "112               [2087.67817, 0.0, 0.0, 62.2058, 0.0]   \n",
       "113                          [0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "114                       [-1, -1, -1, -1, 149.284191]   \n",
       "115                      [-1, -1, 6685.643157, -1, -1]   \n",
       "\n",
       "                                             pred_auth  \\\n",
       "0         [-1, 0.001431, 0.001866, 0.000349, 0.000708]   \n",
       "1            [-1, 1e-05, 0.000349, 0.000131, 0.001755]   \n",
       "2          [0.013005, 0.00123, 0.102042, -1, 0.004394]   \n",
       "3          [0.000502, 0.001755, -1, 0.101401, 7.8e-05]   \n",
       "4         [0.000816, 0.016628, 0.000334, 0.000121, -1]   \n",
       "..                                                 ...   \n",
       "111  [8e-06, 0.000131, 0.000116, 6.500000000000001e...   \n",
       "112  [0.015486000000000003, 0.003158, 0.00074699999...   \n",
       "113  [0.000391, 0.003158, 0.000491, 0.0007469999999...   \n",
       "114                         [-1, -1, -1, -1, 0.001297]   \n",
       "115                         [-1, -1, 0.000193, -1, -1]   \n",
       "\n",
       "                     pred_deg            pred_class  from_textid  \n",
       "0       [-1, 59, 162, 66, 61]    [-1, 10, 6, 10, 9]            0  \n",
       "1        [-1, 6, 66, 68, 127]   [-1, 24, 10, 0, 10]            0  \n",
       "2    [398, 130, 423, -1, 309]  [18, 11, 17, -1, 11]            0  \n",
       "3       [61, 127, -1, 216, 6]   [18, 10, -1, 17, 5]            0  \n",
       "4      [172, 266, 61, 28, -1]     [3, 25, 3, 9, -1]            0  \n",
       "..                        ...                   ...          ...  \n",
       "111        [7, 68, 39, 40, 6]      [0, 0, 6, 3, 24]            9  \n",
       "112  [222, 168, 106, 83, 125]     [25, 18, 5, 5, 3]            9  \n",
       "113    [33, 168, 43, 106, 90]     [9, 18, 21, 5, 5]            9  \n",
       "114      [-1, -1, -1, -1, 20]   [-1, -1, -1, -1, 9]            9  \n",
       "115      [-1, -1, 67, -1, -1]   [-1, -1, 3, -1, -1]            9  \n",
       "\n",
       "[116 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_new2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
